{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chicago_trips_predict_time_new_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Environment (conda_python2)",
      "language": "python",
      "name": "conda_python2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KeFVz6PaoXvN",
        "colab": {}
      },
      "source": [
        "'''Import the required libraries'''\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from google.cloud import bigquery\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cGzvt4uVojps",
        "colab": {}
      },
      "source": [
        "'''Authenticate Colab'''\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LscIZAv1NpJH"
      },
      "source": [
        "# Data Fetching from Big Query\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5fQCzpuOXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Query to fetch sample of data 10% from 100 million records due to RAM constraints: Used Stratified Sampling across pick_community_area such that the sample has same propotion of pickup area as of population'''\n",
        "\n",
        "project_id = 'tidy-ivy-242507'\n",
        "client = bigquery.Client(project_id)\n",
        "\n",
        "query_subsample = \"\"\"SELECT *\n",
        "FROM (\n",
        "  SELECT *  \n",
        "  FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` a \n",
        "   JOIN (\n",
        "  SELECT *, SUM(c) OVER() total\n",
        "  FROM (\n",
        "    SELECT pickup_community_area as pickup_community_area_new , EXTRACT(MONTH FROM trip_start_timestamp) as month_, EXTRACT(YEAR FROM trip_start_timestamp) as year_, COUNT(*) c \n",
        "    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
        "    GROUP BY 1,2,3)) b on a.pickup_community_area = b.pickup_community_area_new and EXTRACT(MONTH FROM a.trip_start_timestamp) = b.month_ and EXTRACT(YEAR FROM a.trip_start_timestamp) = b.year_\n",
        "  WHERE RAND()< 10000000/total\n",
        ") \"\"\"\n",
        "\n",
        "\n",
        "data_sample = client.query(query_subsample).to_dataframe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K-Ei1GQhyuNO",
        "colab": {}
      },
      "source": [
        "'''To get Access to Drive for reading and writing files if needed'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9of4V-726Ht_",
        "colab": {}
      },
      "source": [
        "'''Import from Drive the sample that is saved before'''\n",
        "data_sample = pd.read_csv('/content/gdrive/My Drive/complete_sample_10million.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_E14ZSEHOUgN"
      },
      "source": [
        "#Data Exploration and  Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbd0LOiQA1VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''1) Tried to impute missing values of trip_miles with the help of trip_total and vice-versa but there is not much co-relation so skipped\n",
        "  2) Tried to impute drop_offlocation - > trip_miles + Trip_total + pickup_location using predictive approach but performance is decreasing so not used for final model ''' \n",
        "'''\n",
        "data_sample_dummy = data_sample[(~data_sample.trip_miles.isnull()) & (data_sample.trip_total.isnull())]\n",
        "print(data_sample_dummy.info())\n",
        "impute_trip_total = data_sample.groupby(['dropoff_location','pickup_location'],as_index=False)['trip_total'].mean()\n",
        "impute_trip_total.columns = ['dropoff_location','pickup_location','trip_total_impute']\n",
        "print(impute_trip_total.head())\n",
        "print(impute_trip_total.info())\n",
        "impute_trip_miles = data_sample.groupby(['dropoff_location','pickup_location'],as_index=False)['trip_miles'].mean()\n",
        "impute_trip_miles.columns = ['dropoff_location','pickup_location','trip_miles_impute']\n",
        "print(impute_trip_miles.head())\n",
        "print(impute_trip_miles.info())\n",
        "data_sample_new = pd.merge(data_sample,impute_trip_miles,on=['dropoff_location','pickup_location'])\n",
        "data_sample_new['trip_miles'] = np.where(data_sample_new['trip_miles']==None,data_sample_new['trip_miles_impute'],data_sample_new['trip_miles'])\n",
        "print(data_sample_new.isnull().sum())\n",
        "#impute_trip_total\n",
        "data_sample_new = pd.merge(data_sample,impute_trip_total,on=['dropoff_location','pickup_location'])\n",
        "data_sample_new['trip_total_2'] = np.where(data_sample_new['trip_total']==None,data_sample_new['trip_total_impute'],data_sample_new['trip_total'])\n",
        "print(data_sample_new.isnull().sum()\n",
        "#now handling the missing values of drop_off_location\n",
        "#drop_offlocation - > trip_miles + Trip_total + pickup_location\n",
        "print(len(data_sample.dropoff_location.unique()))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8k8A8fq73f",
        "colab_type": "text"
      },
      "source": [
        "#### Removed nulls for taxi id and trip seconds \n",
        "#### Removed outliers wrt trip seconds \n",
        "#### Data set consisting of only 2013-2016 data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-fsDnClOZRm",
        "colab": {}
      },
      "source": [
        "#print(len(data_sample['taxi_id'].unique()))\n",
        "#print(data_sample['taxi_id'].isnull().any())\n",
        "#print(data_sample[(data_sample['taxi_id'].notnull())].info())\n",
        "#As only 635 nulls are there in data with respect to taxi_id , we Remove those rows\n",
        "def pre_process_data(data_sample):\n",
        "  data_sample = data_sample[~data_sample.taxi_id.isnull()]\n",
        "  print(data_sample['trip_end_timestamp'].isnull().any())\n",
        "  data_sample = data_sample[~data_sample.trip_end_timestamp.isnull()]\n",
        "  print(data_sample['trip_end_timestamp'].isnull().any())\n",
        "  data_sample = data_sample[~data_sample.trip_seconds.isnull()]   #target value is needed for building model, so we eliminate rows where null value is present\n",
        "  data_sample = data_sample[(data_sample.pickup_location != data_sample.dropoff_location)]\n",
        "  sns.boxplot(data_sample['trip_seconds'].values,showfliers = False)\n",
        "  \n",
        "  #filtered outliers with respect to trip_seconds based on percentile\n",
        "  data_sample = data_sample[(data_sample.trip_seconds <= np.percentile(data_sample.trip_seconds, 99.7))&(data_sample.trip_seconds > np.percentile(data_sample.trip_seconds, 5))]\n",
        "  \n",
        "  #Selecting only data from 2013 , 2014,2015,2016 for training\n",
        "  data_sample[\"Datetime_start\"] = pd.to_datetime(data_sample['trip_start_timestamp'],format = '%Y-%m-%d %H:%M:%S')\n",
        "  \n",
        "  data_sample_train = data_sample[(data_sample['Datetime_start'] < datetime.datetime(2017, 1,1,0,0,0 ))]\n",
        "  return data_sample_train\n",
        "  \n",
        "data_sample= pre_process_data(data_sample)\n",
        "\n",
        "#print(data_sample_train.size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJIhZW28rlVU",
        "colab_type": "text"
      },
      "source": [
        "#### Preview to check  data type of different attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36TOKKYGuOXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "continous_features = []\n",
        "high_cardinality_features=[]\n",
        "noisy_features = []\n",
        "categorical_features=[]\n",
        "def get_type_of_variables(products):\n",
        "    for y in products.columns[1:]:\n",
        "        if (products[y].dtype == np.float64):            \n",
        "            continous_features.append(y)\n",
        "        \n",
        "        elif (products[y].dtype == np.int64):\n",
        "            if len(products[y].unique()) > 10000 and len(products[y].unique()) < 30000 :\n",
        "                high_cardinality_features.append(y)\n",
        "            elif len(products[y].unique()) >  30000:\n",
        "                noisy_features.append(y)\n",
        "            else:\n",
        "                categorical_features.append(y)\n",
        "        else:\n",
        "            if len(products[y].unique()) > 10000 and len(products[y].unique()) < 30000 :\n",
        "                high_cardinality_features.append(y)\n",
        "            elif len(products[y].unique()) >  30000:\n",
        "                noisy_features.append(y)\n",
        "            else:\n",
        "                categorical_features.append(y)\n",
        "        print(y)\n",
        "        print(len(data_sample[y].unique()))\n",
        "get_type_of_variables(data_sample)\n",
        "#We could see only 'fare', 'tips', 'tolls', 'extras', 'trip_total', 'trip_seconds', 'trip_miles' are continous values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gzFnBnhlZu4F"
      },
      "source": [
        "#### Trying to check if direct mapping of cordinates is possible with census_tract for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGVzAksTNLGR",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#data_sample['check_condition'] = np.where(data_sample['dropoff_census_tract'] == None,np.where(data_sample['dropoff_community_area']==None,0,1),0)\n",
        "#print(sum(list(data_sample['check_condition'])))\n",
        "#data_sample_dummy = data_sample[~data_sample.dropoff_census_tract.isnull()]\n",
        "#print(data_sample_dummy['dropoff_census_tract'].isnull().sum())\n",
        "#del data_sample_dummy\n",
        "data_sample_dummy = data_sample[(~data_sample.dropoff_census_tract.isnull()) & (data_sample.dropoff_longitude.isnull())]\n",
        "#print(data_sample_dummy.isnull().sum())\n",
        "del data_sample_dummy\n",
        "#So even if we impute by direct mapping of dropoff_census_tract to dropoff_longitude only 4959 can be mapped out of 246166\n",
        "#So not fruitful\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7AlXFn04Z_Y2"
      },
      "source": [
        "###binning the less frequent values to one group for company column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nrf_b2aO5wpX",
        "colab": {}
      },
      "source": [
        "def binning_values(data_sample):\n",
        "  company_distribution = data_sample['company'].value_counts(normalize=True).sort_values(ascending=False).reset_index()\n",
        "  company_distribution.columns = ['company','count']\n",
        "  print(company_distribution.info())\n",
        "  company_distribution['cum_sum'] = company_distribution['count'].cumsum()\n",
        "  company_distribution['cum_sum'].plot(style='.-')\n",
        "  company_distribution['count'].plot(style='.-')\n",
        "  #Based on the above plot  we take 13 top companies contributing to demand and rest all are treated as Others category\n",
        "  company_distribution['bin_company'] = np.where(company_distribution['cum_sum'] < 0.99,company_distribution['company'],'Others') \n",
        "  map_company_bins =dict(zip(company_distribution.company,company_distribution.bin_company))\n",
        "\n",
        "  company_map_df = pd.DataFrame()\n",
        "  company_map_df['company'] = list(data_sample[\"company\"].unique())\n",
        "  company_map_df['company_bin'] = company_map_df[\"company\"].map(map_company_bins)\n",
        "  #company_map_df.to_csv('company_bin_map.csv',index=False)\n",
        "  data_sample['company_bin'] = data_sample[\"company\"].map(map_company_bins)\n",
        "  data_sample.drop(['company'],axis=1,inplace=True)\n",
        "  #Get Relationship between company and taxi_id if any\n",
        "  \n",
        "  get_sample_company_not_null = data_sample[~data_sample.company_bin.isnull()]\n",
        "  map_taxi_id_and_company =dict(zip(get_sample_company_not_null.taxi_id,get_sample_company_not_null.company_bin))\n",
        "  del get_sample_company_not_null\n",
        "  data_sample[\"company_new\"] = data_sample[\"taxi_id\"].map(map_taxi_id_and_company)\n",
        "  data_sample['company_bin'].fillna(data_sample['company_new'],inplace=True)\n",
        "  data_sample.drop(['company_new'],axis=True,inplace=True)\n",
        "  #print(data_sample.isnull().sum())\n",
        "  data_sample['company_bin'].fillna('Other_2',inplace=True)\n",
        "  data_sample.dropna(inplace=True)\n",
        "  return data_sample\n",
        "data_sample = binning_values(data_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5CbBeA0w56MT",
        "colab": {}
      },
      "source": [
        "company_distribution.to_csv('company_distribution.csv',index=False)\n",
        "\n",
        "!cp company_distribution.csv /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WuzYcCFlGXpw",
        "colab": {}
      },
      "source": [
        "#Imputing Null values\n",
        "#1) trip_miles can be predicted using pickup_location and dropoff_location \n",
        "#Here try for relation among null variables and non null values\n",
        "#for now removing all null values and imputing some nulls using 'Other_2' category\n",
        "#print(data_sample.info())\n",
        "\n",
        "#print(data_sample.info())\n",
        "#print(data_sample.isnull().sum())\n",
        "\n",
        "'''Null values deleted now for all the columns'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3gFK2lvsBAz",
        "colab_type": "text"
      },
      "source": [
        "### Clustering latitude , longitude Co-ordinates  of Chicago City where pickup and drop off happened"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm2Q5AB8uOYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict_clusters for Dropoff_location and Pick_up_location\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import pickle\n",
        "cluster = MiniBatchKMeans(n_clusters=15, max_iter=1000, batch_size=100000, verbose=0, compute_labels=True, \n",
        "                          random_state=None, tol=0.0, max_no_improvement=10, n_init=3, reassignment_ratio=0.005)\n",
        "df1 = data_sample[['pickup_longitude', 'pickup_latitude']].as_matrix()\n",
        "df2 = data_sample[['dropoff_longitude', 'dropoff_latitude']].as_matrix()\n",
        "features = np.round(np.vstack([df1, df2]), 5)\n",
        "cluster.fit(features)\n",
        "data_sample['pickup_cluster_label'] = cluster.predict(data_sample[['pickup_longitude', 'pickup_latitude']])\n",
        "data_sample['dropoff_cluster_label'] = cluster.predict(data_sample[['dropoff_longitude', 'dropoff_latitude']])\n",
        "\n",
        "pickle.dump(cluster, open( \"cluster.p\", \"wb\" ))\n",
        "!cp cluster.p /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-fYW98ouOYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_ = plt.hist(cluster.labels_, bins=50)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U53rJGchsfol",
        "colab_type": "text"
      },
      "source": [
        "#### binning the less frequent values to one group for drop off location column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrO5AOCMQUPU",
        "colab": {}
      },
      "source": [
        "dropoff_location_distribution = data_sample['dropoff_location'].value_counts(normalize=True).sort_values(ascending=False).reset_index()\n",
        "dropoff_location_distribution.columns = ['dropoff_location','count']\n",
        "print(dropoff_location_distribution.info())\n",
        "dropoff_location_distribution['cum_sum'] = dropoff_location_distribution['count'].cumsum()\n",
        "dropoff_location_distribution['cum_sum'].plot(style='.-')\n",
        "dropoff_location_distribution['count'].plot(style='.-')\n",
        "#dropoff_location_distribution.to_csv('dropoff_location_distribution.csv',index=False)\n",
        "\n",
        "dropoff_location_distribution['bin_dropoff_location'] = np.where(dropoff_location_distribution['cum_sum'] < 0.95,dropoff_location_distribution['dropoff_location'],'Others')\n",
        "#print(company_distribution.head(15))\n",
        "map_dropoff_location_bins =dict(zip(dropoff_location_distribution.dropoff_location,dropoff_location_distribution.bin_dropoff_location))\n",
        "data_sample['dropoff_location_bin'] = data_sample[\"dropoff_location\"].map(map_dropoff_location_bins)\n",
        "data_sample.drop(['dropoff_location'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nudubcopuOYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropoff_location_distribution.to_csv('dropoff_location_distribution.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "erQVVlRkdJ_e",
        "colab": {}
      },
      "source": [
        "!cp dropoff_location_distribution.csv /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFow1luCsvYF",
        "colab_type": "text"
      },
      "source": [
        "#### binning the less frequent values to one group for pick up location column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3YGTK1oTGTrD",
        "colab": {}
      },
      "source": [
        "pickup_location_distribution = data_sample['pickup_location'].value_counts(normalize=True).sort_values(ascending=False).reset_index()\n",
        "pickup_location_distribution.columns = ['pickup_location','count']\n",
        "print(pickup_location_distribution.info())\n",
        "#pickup_census_tract_distribution.index = pickup_census_tract_distribution['pickup_census_tract']\n",
        "#Plot the pickup_census to know the actual distribution so that we can bin non- frequent categories\n",
        "#print(pickup_census_tract_distribution.head())\n",
        "pickup_location_distribution['cum_sum'] = pickup_location_distribution['count'].cumsum()\n",
        "pickup_location_distribution['cum_sum'].plot(style='.-')\n",
        "pickup_location_distribution['count'].plot(style='.-')\n",
        "pickup_location_distribution.to_csv('pickup_location_distribution.csv',index=False)\n",
        "#!cp pickup_location_distribution.csv /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oh_J_yOUe6WB",
        "colab": {}
      },
      "source": [
        "pickup_location_distribution['bin_pickup_location'] = np.where(pickup_location_distribution['cum_sum'] < 0.95,pickup_location_distribution['pickup_location'],'Others')\n",
        "#print(company_distribution.head(15))\n",
        "map_pickup_location_bins =dict(zip(pickup_location_distribution.pickup_location,pickup_location_distribution.bin_pickup_location))\n",
        "data_sample['pickup_location_bin'] = data_sample[\"pickup_location\"].map(map_pickup_location_bins)\n",
        "data_sample.drop(['pickup_location'],axis=1,inplace=True)\n",
        "#print(data_sample.info())\n",
        "data_sample = data_sample[['unique_key','taxi_id','pickup_cluster_label','dropoff_cluster_label','company_bin','payment_type','dropoff_location_bin','pickup_location_bin','trip_seconds','trip_miles','fare','tolls','trip_total','trip_start_timestamp','trip_end_timestamp']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dR0qqDYcgeuC"
      },
      "source": [
        "# Pre Processing and Impute Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JiSr4NnBgtE5",
        "colab": {}
      },
      "source": [
        "data_sample[\"Datetime_start\"] = pd.to_datetime(data_sample['trip_start_timestamp'],format = '%Y-%m-%d %H:%M:%S')\n",
        "data_sample[\"Month\"] = data_sample.Datetime_start.dt.month\n",
        "data_sample[\"Week\"] = data_sample.Datetime_start.dt.week\n",
        "data_sample[\"DayofWeek\"] = data_sample.Datetime_start.dt.dayofweek\n",
        "data_sample[\"hour_start\"] = data_sample.Datetime_start.dt.hour\n",
        "data_sample[\"year\"] = data_sample.Datetime_start.dt.year\n",
        "data_sample[\"day\"] = data_sample.Datetime_start.dt.day\n",
        "data_sample.drop(['trip_start_timestamp','Datetime_start'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cprARYkZhYR4"
      },
      "source": [
        "### Imputing Traffic Per Hour across Date , Hour and Drop off Cluster Level\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygr0z48juOY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict Traffic Area: \n",
        "#1) We first take demand from pickup point hourly time across different dropoff clusters \n",
        "#2) calculate pickup point hourly time demand  \n",
        "#3) divide 1 by 2 to get hourly traffic\n",
        "#here we are getting demand at hour pickup cluster level wise\n",
        "cols = ['unique_key', 'Month', 'day','year', 'hour_start', 'dropoff_cluster_label']\n",
        "grp = data_sample[cols].groupby(['Month', 'day', 'year','hour_start', \n",
        "                          'dropoff_cluster_label']).agg('count')\n",
        "grp.columns = [x+'_count' for x in grp.columns]\n",
        "grp.reset_index(inplace=True)\n",
        "print(grp.head())\n",
        "data_sample_grp = pd.merge(data_sample,grp,on=['Month', 'day', 'year','hour_start', \n",
        "                          'dropoff_cluster_label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGSHGq5MuOY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "del grp\n",
        "#Similarly getting demand at hour level\n",
        "grp2 = data_sample[cols].groupby(['Month', 'day', 'year','hour_start' ]).agg('count')\n",
        "grp2.columns = [x+'_count_hr' for x in grp2.columns]\n",
        "grp2.reset_index(inplace=True)\n",
        "del data_sample\n",
        "data_sample_grp_new = pd.merge(data_sample_grp,grp2,on=['Month', 'day', 'year','hour_start'])\n",
        "#hourly traffic demand\n",
        "data_sample_grp_new['traffic_hr_cluster'] = data_sample_grp_new['unique_key_count']/data_sample_grp_new['unique_key_count_hr']*1.0\n",
        "print(data_sample_grp_new.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbKZ_xQiuOY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sample = data_sample_grp_new\n",
        "del data_sample_grp_new\n",
        "del data_sample_grp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C891gLLtHlj",
        "colab_type": "text"
      },
      "source": [
        "### Bin Hour data based on demand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ0Z12NQ25PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "trips_by_hour =pd.DataFrame(data_sample.groupby('hour_start').size().reset_index(name = \"demand\"))\n",
        "trips_by_hour.plot(x='hour_start', y='demand', kind='bar', figsize=(8,6))\n",
        "plt.title(\"Count of Taxi Rides Per Time of Day\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Emz9c6eWhgT7",
        "colab": {}
      },
      "source": [
        "#Group the hour column  based on the demand from the above mentioned plot\n",
        "\n",
        "def getHour_num(x):    \n",
        "    if x >= 2 and x<=7:\n",
        "        y = 1  \n",
        "    \n",
        "    elif x>=8 and x<=11:\n",
        "        y = 2\n",
        "    \n",
        "    elif x>=12 and x<= 16:\n",
        "        y = 3 \n",
        "    \n",
        "    elif x>=17 and x<=20:\n",
        "        y = 4\n",
        "    \n",
        "    elif x>=20 and x<=23:\n",
        "        y = 5\n",
        "      \n",
        "    elif x>=0 and x<=1:\n",
        "        y = 0\n",
        "    \n",
        "    return y\n",
        "data_sample[\"hour_bin\"] = data_sample[\"hour_start\"].apply((lambda x : getHour_num(x)))\n",
        "#data_sample.drop(['hour_start'],axis=1,inplace=True)\n",
        "print(data_sample.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1VbLmK8uOZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''To know trip_seconds and trip duration distributions'''\n",
        "hours = np.array(list(data_sample.hour_start))\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.boxplot(hours,data_sample['trip_seconds'].values/3600,showfliers = False)\n",
        "plt.title('Distribution of trip duration depending on hour of the day')\n",
        "plt.ylabel('Trip duration in hours')\n",
        "plt.xlabel('Hour of day from 0:00 to 23:00')\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.boxplot(hours,data_sample['trip_miles'],showfliers = False)\n",
        "plt.title('Distribution of trip distance depending on hour of the day')\n",
        "plt.ylabel('Trip duration in hours')\n",
        "plt.xlabel('Hour of day from 0:00 to 23:00')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WYmfuAfmh2Dy"
      },
      "source": [
        "###Using Weather Conditions from Kaggle dataset as features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ioXwUd5iCqp",
        "colab": {}
      },
      "source": [
        "'''Getting weather conditions hourly from this link https://www.kaggle.com/selfishgene/historical-hourly-weather-data#humidity.csv'''\n",
        "humidity = pd.read_csv('humidity.csv')\n",
        "pressure = pd.read_csv('pressure.csv')\n",
        "temperature = pd.read_csv('temperature.csv')\n",
        "wind_direction = pd.read_csv('wind_direction.csv')\n",
        "wind_speed = pd.read_csv('wind_speed.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lHMLXKOtkII",
        "colab_type": "text"
      },
      "source": [
        "#### Imputing Missing Values of weather condition using exponential weighted average around 12 hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PPZ3yjyxF7Ln",
        "colab": {}
      },
      "source": [
        "\n",
        "'''Imputing Missing Values of weather condition using exponential weighted average around 12 hours'''\n",
        "humidity_ts = humidity[['datetime','Chicago']]\n",
        "humidity_ts.columns = ['datetime','humidity']\n",
        "humidity_ts['exp_humidity'] =humidity_ts['humidity'].ewm(span=12).mean()\n",
        "\n",
        "humidity_ts['humidity_imp'] = np.where(humidity_ts['humidity'].isnull(),humidity_ts['exp_humidity'],humidity_ts['humidity'])\n",
        "humidity_ts.drop(['humidity','exp_humidity'],axis=1,inplace=True)\n",
        "\n",
        "pressure_ts = pressure[['datetime','Chicago']]\n",
        "pressure_ts.columns = ['datetime','pressure']\n",
        "pressure_ts['exp_pressure'] =pressure_ts['pressure'].ewm(span=12).mean()\n",
        "\n",
        "pressure_ts['pressure_imp'] = np.where(pressure_ts['pressure'].isnull(),pressure_ts['exp_pressure'],pressure_ts['pressure'])\n",
        "pressure_ts.drop(['pressure','exp_pressure'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "temperature_ts = temperature[['datetime','Chicago']]\n",
        "temperature_ts.columns = ['datetime','temperature']\n",
        "temperature_ts['exp_temperature'] =temperature_ts['temperature'].ewm(span=12).mean()\n",
        "\n",
        "temperature_ts['temperature_imp'] = np.where(temperature_ts['temperature'].isnull(),temperature_ts['exp_temperature'],temperature_ts['temperature'])\n",
        "temperature_ts.drop(['temperature','exp_temperature'],axis=1,inplace=True)\n",
        "\n",
        "wind_direction_ts = wind_direction[['datetime','Chicago']]\n",
        "wind_direction_ts.columns = ['datetime','wind_direction']\n",
        "wind_direction_ts['exp_wind_direction'] =wind_direction_ts['wind_direction'].ewm(span=12).mean()\n",
        "\n",
        "wind_direction_ts['wind_direction_imp'] = np.where(wind_direction_ts['wind_direction'].isnull(),wind_direction_ts['exp_wind_direction'],wind_direction_ts['wind_direction'])\n",
        "\n",
        "wind_direction_ts.drop(['wind_direction','exp_wind_direction'],axis=1,inplace=True)\n",
        "\n",
        "wind_speed_ts = wind_speed[['datetime','Chicago']]\n",
        "wind_speed_ts.columns = ['datetime','wind_speed']\n",
        "wind_speed_ts['exp_wind_speed'] =wind_speed_ts['wind_speed'].ewm(span=12).mean()\n",
        "\n",
        "wind_speed_ts['wind_speed_imp'] = np.where(wind_speed_ts['wind_speed'].isnull(),wind_speed_ts['exp_wind_speed'],wind_speed_ts['wind_speed'])\n",
        "\n",
        "wind_speed_ts.drop(['wind_speed','exp_wind_speed'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "print(wind_speed_ts.head())\n",
        "print(data_sample.head())\n",
        "\n",
        "#trying to fill missing values using weighted average\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aODs6Cw4URSi",
        "colab": {}
      },
      "source": [
        "'''Merge all weather columns in a single dataframe '''\n",
        "file_1 = pd.merge(humidity_ts,pressure_ts,on=['datetime'])\n",
        "file_2 = pd.merge(file_1,temperature_ts,on=['datetime'])\n",
        "file_3 = pd.merge(file_2,wind_direction_ts,on=['datetime'])\n",
        "file_weather = pd.merge(file_3,wind_speed_ts,on=['datetime'])\n",
        "del file_1\n",
        "del file_2\n",
        "del file_3\n",
        "del humidity_ts\n",
        "del pressure_ts\n",
        "del temperature_ts\n",
        "del wind_direction_ts\n",
        "del wind_speed_ts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCgpU8yOMfgO",
        "colab": {}
      },
      "source": [
        "'''Convert timestamp to month , hour , day , year to join with our parent dataset'''\n",
        "def convert(data,data_sample):\n",
        "  data[\"Datetime_start\"] = pd.to_datetime(data['datetime'],format = '%Y-%m-%d %H:%M:%S')\n",
        "  data[\"Month\"] = data.Datetime_start.dt.month\n",
        "  data[\"hour_start\"] = data.Datetime_start.dt.hour\n",
        "  data[\"year\"] = data.Datetime_start.dt.year\n",
        "  data[\"day\"] = data.Datetime_start.dt.day\n",
        "  #data.drop(['Datetime_start'],axis=1,inplace=True)\n",
        "  data_sample_new = pd.merge(data_sample,data,on=['Month','year','day','hour_start'])\n",
        "  del data_sample\n",
        "  del data\n",
        "  return data_sample_new\n",
        "data_sample_new = convert(file_weather,data_sample)\n",
        "data_sample_new.sort_values(by= 'Datetime_start',inplace=True)\n",
        "data_sample_new.drop(['Datetime_start'],axis=1,inplace=True)\n",
        "del data_sample\n",
        "del file_weather"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKDM6NpuOZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop off location insights\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.boxplot(np.array(list(data_sample_new['dropoff_location_bin'])),data_sample_new['trip_seconds'].values/3600,showfliers = False)\n",
        "plt.title('Distribution of trip seconds depending on drop off location')\n",
        "plt.ylabel('Trip duration in hours')\n",
        "plt.xlabel('dropoff_location_bin')\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.boxplot(np.array(list(data_sample_new['pickup_location_bin'])),data_sample_new['trip_seconds'].values/3600,showfliers = False)\n",
        "plt.title('Distribution of trip seconds depending on drop off location')\n",
        "plt.ylabel('Trip duration in hours')\n",
        "plt.xlabel('dropoff_location_bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypOCyHy56cAy"
      },
      "source": [
        "# Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XL37khyY6sTZ",
        "colab": {}
      },
      "source": [
        " \n",
        "data_sample_new.drop(['day','year','hour_start','trip_end_timestamp','payment_type','datetime'],axis=1,inplace=True)\n",
        "\n",
        "#So we take only these columns to predict our trip_seconds target variable\n",
        "data = data_sample_new[['taxi_id','company_bin','pickup_cluster_label','dropoff_cluster_label',\n",
        "       'dropoff_location_bin', 'pickup_location_bin', 'trip_seconds',\n",
        "       'trip_miles', 'fare', 'tolls', 'trip_total', \n",
        "       'Month', 'Week', 'DayofWeek', \n",
        "       'hour_bin','traffic_hr_cluster', 'humidity_imp', 'pressure_imp',\n",
        "       'temperature_imp', 'wind_direction_imp', 'wind_speed_imp']]\n",
        " \n",
        "del data_sample_new\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7cgnc4MOHoyZ",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#Clear Memory\n",
        "del dropoff_location_distribution\n",
        "del map_company_bins\n",
        "del humidity\n",
        "del map_dropoff_location_bins\n",
        "del map_pickup_location_bins\t \n",
        "del map_taxi_id_and_company\n",
        "del pickup_location_distribution\t \n",
        "del pressure\n",
        "del temperature\n",
        "del wind_direction\n",
        "del wind_speed\n",
        "del categorical_features\t\n",
        "del company_distribution\t\n",
        "del continous_features\n",
        "del high_cardinality_features\t \n",
        "del get_type_of_variables\t \n",
        "del getHour_num\n",
        "%who\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnvGj5htt5hk",
        "colab_type": "text"
      },
      "source": [
        "#### Encode categorical variables and then convert into one hot encoding for feeding to the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bgoVlgYmCM_0",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler, LabelEncoder\n",
        "\n",
        "'''Encode categorical variables and then convert into one hot encoding for feeding to the model '''\n",
        "def ohe_features(data): \n",
        "    columns = ['company_bin', 'pickup_cluster_label','dropoff_cluster_label','dropoff_location_bin', 'pickup_location_bin',\n",
        "       'Month', 'Week', 'DayofWeek', 'hour_bin']\n",
        "    for f in columns :\n",
        "        #print(f)\n",
        "        enc = LabelEncoder()\n",
        "        data[f]=enc.fit_transform(data[f])\n",
        "        pickle.dump(enc,open('enc_'+str(f)+'.h',\"wb\"))\n",
        "    \n",
        "    df_class_num =[12,15,15,80,59,12,53,7,6]\n",
        "    ohe = OneHotEncoder(n_values = df_class_num, dtype=np.int32, sparse=False)\n",
        "    \n",
        "    X_train_1 = ohe.fit_transform(data[columns].values)\n",
        "    pickle.dump(ohe,open('ohe_trip_cluster_traffic.h',\"wb\"))\n",
        "    del data\n",
        "    return X_train_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzcGZ9TMuEPk",
        "colab_type": "text"
      },
      "source": [
        "### Scale the continous columns and join with the categorical variables to feed to the model'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uR19EUaxCS-e",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "'''Scale the continous columns and join with the categorical variables to feed to the model'''\n",
        "def final_process(processed,data_processed):\n",
        "    scaler = StandardScaler(with_mean=True)\n",
        "    continous_columns = ['trip_miles','traffic_hr_cluster', 'tolls', 'trip_total','humidity_imp', 'pressure_imp', 'temperature_imp']\n",
        "    #data_processed[continous_columns] = data_processed[continous_columns].fillna(0)\n",
        "    \n",
        "    scaled_columns = scaler.fit_transform(data_processed[continous_columns])\n",
        "    scaler_filename = \"scaler_trip_cluster_traffic.save\"\n",
        "    joblib.dump(scaler, scaler_filename) \n",
        "    \n",
        "    target_variable = data_processed['trip_seconds']\n",
        "    del data_processed\n",
        "    del scaler\n",
        "    del continous_columns\n",
        "    \n",
        "    processed_data_new = np.concatenate([processed, scaled_columns], axis=1)\n",
        "    del processed    \n",
        "    #del processed_data_this\n",
        "    del scaled_columns\n",
        "    return processed_data_new, target_variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k86ySeSUQI7o",
        "colab": {}
      },
      "source": [
        "del bigquery\n",
        "del noisy_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cJN9qWXgCYLH",
        "colab": {}
      },
      "source": [
        "#Run the above methods and return the training data set and training target column'''\n",
        "def final_data_to_model(data_processed):\n",
        "    processed = ohe_features(data_processed)          \n",
        "    processed_data_new, target_variable = final_process(processed,data_processed)\n",
        "    del processed\n",
        "    del data_processed\n",
        "    return processed_data_new, target_variable\n",
        "data1, target1 = final_data_to_model(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uquioAKUuOZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''The total number of rows for training were around 5 million after removing outliers and null values of target and cordinates columns from 8 million'''\n",
        "data_train = data1[0 :3800000]\n",
        "data_validation = data1[3800000:]\n",
        "target_train = target1[0:3800000]\n",
        "target_validation = target1[3800000:]\n",
        "del data1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nk0eZphuN5f",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Model using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITIQMkRzuOZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout\n",
        "from keras import regularizers\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "'''Input layer used Relu as activation functions in inout and hidden layers and used he_normal weights initialization'''\n",
        "model.add(Dense(130, input_dim=266, kernel_initializer='he_normal', activation='relu'))\n",
        "'''Hidden Layer we did batch normalization for better training convergence and dropout for generalization'''\n",
        "model.add(Dense(65,kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "print('fitting model')\n",
        "'''Used ADAM optimizer for better weight updations '''\n",
        "model.compile(loss='mean_absolute_percentage_error', optimizer='adam')\n",
        "model.fit(data_train, target_train, epochs=50,batch_size=2048,verbose=1,shuffle=True,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoOhyOsYuOZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save Model for prediction\n",
        "model.save('trip_output_cluster_traffic.h5')\n",
        "model.save_weights('trip_weights_new_cluster_traffic.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbfO08duViG",
        "colab_type": "text"
      },
      "source": [
        "### Compute MAPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tth01RlPuOZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict Validation dataset \n",
        "predicted_validation =  model.predict(data_validation)\n",
        "\n",
        "predicted = predicted_validation\n",
        "predicted_new = []\n",
        "for value in predicted:\n",
        "    predicted_new.append(value[0])\n",
        "\n",
        "'''Compute MAPE'''\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "result = mean_absolute_percentage_error(target_validation, predicted_new)\n",
        "print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}